{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similitud de documentos (I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar dataset generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import zlib\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import config\n",
    "\n",
    "# Import\n",
    "with open(config.DATASET_MUCHOCINE, 'r') as fd:\n",
    "    documents = json.loads(zlib.decompress(fd.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_vect = DictVectorizer()\n",
    "docs_vect = model_vect.fit_transform([Counter(doc['body_tokens']) for doc in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Documentos:{}\\nPalabras:{}\".format(*docs_vect.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert(len(documents[0]['body_tokens'])==docs_vect[0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print docs_vect[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print docs_vect[0].toarray()  # Sparse representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud entre documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_most_similars(doc_id, docs_vect, n=1):\n",
    "    '''Return the most (n) similar documents from the list\n",
    "    '''\n",
    "    rank = sorted(enumerate(cosine_similarity(docs_vect[doc_id], docs_vect)[0]), reverse=True, key= lambda (_,similarity): similarity)\n",
    "    rank = [x for x in rank if x[0]!=doc_id] # Filter original doc\n",
    "    return rank[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "find_most_similars(0, docs_vect, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_sims = cosine_similarity(docs_vect, docs_vect)  # NxN document similarity matrix\n",
    "np.fill_diagonal(all_sims,0)  # The most similar document is itself, set to 0\n",
    "most_similar = all_sims.argmax(axis=1)\n",
    "similarities = []  # (doc,doc, similarity) for most similar document to each one\n",
    "for n_doc in xrange(len(documents)):\n",
    "    sim_doc = most_similar[n_doc]\n",
    "    similarities.append( (n_doc, sim_doc, all_sims[n_doc, sim_doc]) )\n",
    "similarities.sort(key=lambda x:x[2], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "similarities[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que hay documentos muy parecidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Quitar duplicados, mismo texto y autor\n",
    "\n",
    "similarities = [\n",
    "    (doc1,doc2,sim)\n",
    "    for doc1,doc2,sim in similarities\n",
    "    if not (\n",
    "        documents[doc1]['body'] == documents[doc2]['body'] and\n",
    "        documents[doc1]['author'] == documents[doc2]['author']\n",
    "    )\n",
    "]\n",
    "\n",
    "similarities[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot([x[2] for x in similarities])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontrar copiones de comentarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_copycats(similarities, threshold):\n",
    "    \"\"\"Documentos muy parecidos con diferentes autores\"\"\"\n",
    "    retval = []\n",
    "    for doc1,doc2,score in similarities:\n",
    "        if score>threshold and documents[doc1]['author']!=documents[doc2]['author']:\n",
    "            retval.append((doc1,doc2,score))\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "find_copycats(similarities,0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for doc1,doc2,score in find_copycats(similarities,0.65):\n",
    "    print u\"Authors: {}-{}, {}\".format(documents[doc1]['author'], documents[doc2]['author'], score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Los dos documentos más parecidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_doc(doc):\n",
    "    print doc['author']\n",
    "    print doc['title']\n",
    "    print doc['body']\n",
    "\n",
    "doc1, doc2, score = [s for s in similarities if s[2]<0.65][0]\n",
    "print \"Score: {}\\n\".format(score)\n",
    "print_doc(documents[doc1])\n",
    "print \"\"\n",
    "print_doc(documents[doc2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución de consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Función de pasar textos a tokens, del cuaderno anterior\n",
    "\n",
    "STOPWORDS = set('''\n",
    "a al algo algunas algunos ante antes como con contra cual cuando de del desde donde durante e el ella ellas ellos\n",
    "en entre era erais eran eras eres es esa esas ese eso esos esta estaba estabais estaban estabas estad estada estadas\n",
    "estado estados estamos estando estar estaremos estara estaran estaras estare estareis estaria estariais estariamos\n",
    "estarian estarias estas este estemos esto estos estoy estuve estuviera estuvierais estuvieran estuvieras estuvieron\n",
    "estuviese estuvieseis estuviesen estuvieses estuvimos estuviste estuvisteis estuvieramos estuviesemos estuvo esta\n",
    "estabamos estais estan estas este esteis esten estes fue fuera fuerais fueran fueras fueron fuese fueseis fuesen fueses\n",
    "fui fuimos fuiste fuisteis fueramos fuesemos ha habida habidas habido habidos habiendo habremos habra habran habras\n",
    "habre habreis habria habriais habriamos habrian habrias habeis habia habiais habiamos habian habias han has hasta\n",
    "hay haya hayamos hayan hayas hayais he hemos hube hubiera hubierais hubieran hubieras hubieron hubiese hubieseis\n",
    "hubiesen hubieses hubimos hubiste hubisteis hubieramos hubiesemos hubo la las le les lo los me mi mis mucho muchos\n",
    "muy mas mi mia mias mio mios nada ni no nos nosotras nosotros nuestra nuestras nuestro nuestros o os otra otras otro\n",
    "otros para pero poco por porque que quien quienes que se sea seamos sean seas sentid sentida sentidas sentido sentidos\n",
    "seremos sera seran seras sere sereis seria seriais seriamos serian serias seais siente sin sintiendo sobre sois somos\n",
    "son soy su sus suya suyas suyo suyos si tambien tanto te tendremos tendra tendran tendras tendre tendreis tendria\n",
    "tendriais tendriamos tendrian tendrias tened tenemos tenga tengamos tengan tengas tengo tengais tenida tenidas tenido\n",
    "tenidos teniendo teneis tenia teniais teniamos tenian tenias ti tiene tienen tienes todo todos tu tus tuve tuviera\n",
    "tuvierais tuvieran tuvieras tuvieron tuviese tuvieseis tuviesen tuvieses tuvimos tuviste tuvisteis tuvieramos tuviesemos\n",
    "tuvo tuya tuyas tuyo tuyos tu un una uno unos vosostras vosostros vuestra vuestras vuestro vuestros y ya yo el eramos\n",
    "'''.split())\n",
    "\n",
    "from nltk.stem.snowball import SpanishStemmer\n",
    "stemmer = SpanishStemmer()\n",
    "\n",
    "def txt2words(txt):\n",
    "    txt = txt.lower()  # Text in lowercase\n",
    "    table = dict(zip( #  Quitar tildes\n",
    "        [ord(x) for x in u'áéíóúü'],\n",
    "        [ord(x) for x in u'aeiouu']\n",
    "    ))\n",
    "    txt = txt.translate(table)    \n",
    "    txt = ''.join([\n",
    "        letter for letter in txt \n",
    "        if letter in set(u'abcdefghijklmnñopqrstuvwxyz0123456789 ')]\n",
    "    )\n",
    "    words = [\n",
    "        stemmer.stem(w)\n",
    "        for w in txt.split(' ')\n",
    "        if w!='' and w not in STOPWORDS\n",
    "    ]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_vect(text):\n",
    "    doc = model_vect.transform(Counter(txt2words(text)))\n",
    "    similarities = cosine_similarity(doc, docs_vect)\n",
    "    doc_id,score = similarities.argmax(), similarities.max()    \n",
    "    print doc_id\n",
    "    print u\"Pelicula: '{}'\\nPuntuacion:{}\".format(documents[doc_id]['title'],score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tematica correcta\n",
    "query_vect(u'quiero una pelicula de boxeo')\n",
    "query_vect(u'quiero una peli de boxeo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Understemming\n",
    "query_vect(u'quiero una pelicula triste')\n",
    "query_vect(u'quiero una peli triste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_vect(u'Algo de rambo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_vect(u'ciencia ficción futurista')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_vect(u'amor romántico bobo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_vect(u'americanada universidad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_tfidf = TfidfTransformer()\n",
    "docs_tfidf = model_tfidf.fit_transform(docs_vect)\n",
    "\n",
    "def query_tfidf(text):\n",
    "    doc,score = query_documents(text, model_tfidf, docs_tfidf)\n",
    "    print doc\n",
    "    print u\"Pelicula: '{}'\\nPuntuacion:{}\".format(documents[doc]['title'],score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_tfidf(text):\n",
    "    doc = model_tfidf.transform(  # IDF learn\n",
    "            model_vect.transform(  # Matrix representation\n",
    "                Counter(txt2words(text))  # Dict of frequencies\n",
    "            )\n",
    "    )\n",
    "    similarities = cosine_similarity(doc, docs_tfidf)\n",
    "    doc_id,score = similarities.argmax(), similarities.max()    \n",
    "    print doc_id\n",
    "    print u\"Pelicula: '{}'\\nPuntuacion:{}\".format(documents[doc_id]['title'],score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalizacion peli -pelicula\n",
    "query_tfidf(u'quiero una pelicula de boxeo')\n",
    "query_tfidf(u'quiero una peli de boxeo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Understemming\n",
    "query_tfidf(u'quiero una pelicula triste')\n",
    "query_tfidf(u'quiero una peli triste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_tfidf(u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_tfidf(u'ciencia ficción futurista')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
