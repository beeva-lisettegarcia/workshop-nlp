{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Palabras clave (1ª versión)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el primer cuaderno de la presentación [Text mining is Not Magic](https://docs.google.com/presentation/d/15l5MarIFXz67HqtSoqYEGLTioNt_D5F7YPuMIB4EvIU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frecuencia de aparición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_keywords(words):\n",
    "    '''Return the top10 frequent words.\n",
    "    '''\n",
    "    return [\n",
    "        x for x,_ in \n",
    "        sorted(\n",
    "            Counter(words).iteritems(),\n",
    "            key=lambda (_,freq):freq,\n",
    "            reverse=True\n",
    "        )\n",
    "    ][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print get_keywords(['esta', 'es', 'una', 'prueba', 'para', 'ver', 'si', 'es', 'correcta', 'la', 'función', 'para', 'calcular', 'frecuencias'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset URL: http://www.lsi.us.es/~fermin/corpusCine.zip\n",
    "\n",
    "Nos quedamos con los ficheros .xml, que tienen una estructura del estilo:\n",
    "\n",
    "```xml\n",
    "<review author=\"XXX\" title=\"XXX\" rank=\"X\" maxRank=\"X\" source=\"XXX\">\n",
    "\t<summary>XXX</summary>\n",
    "\t<body>XXX</body>\n",
    "</review>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detección de la codificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "with open(config.DATASET_MUCHOCINE_RAW+'/100.xml', 'r') as fd:\n",
    "    txt = fd.read()\n",
    "    \n",
    "print chardet.detect(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parseado con expresiones regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs  # Know your encoding\n",
    "import re\n",
    "\n",
    "def parse_file(file_path):\n",
    "    retval = {'file': os.path.basename(file_path)}\n",
    "    with codecs.open(file_path,'r', encoding='ISO-8859-2') as fd:\n",
    "        f = fd.read().strip()\n",
    "    reg_expr = re.compile(r'\\<review author=\"(?P<author>.*)\" title=\"(?P<title>.*)\" rank=\"(?P<rank>\\d)\".*\\>\\s*<summary>(?P<summary>.*)</summary>\\s*<body>(?P<body>.*)</body>\\s*</review>')\n",
    "    regexp_result = reg_expr.search(f.strip())\n",
    "    for key in ['author', 'rank', 'title', 'summary', 'body']:\n",
    "        retval[key] = regexp_result.group(key)\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "for file_name in os.listdir(config.DATASET_MUCHOCINE_RAW):\n",
    "    try:\n",
    "        documents.append(\n",
    "            parse_file(config.DATASET_MUCHOCINE_RAW+'/'+file_name)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print file_name\n",
    "        print e.message\n",
    "documents.sort(key=lambda x: x['file'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documento de ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_document = documents[1478]  # 'Harry Potter y la piedra filosofal'\n",
    "print my_document['title']\n",
    "print my_document['body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ª versión. Contar palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def txt2words1(txt):\n",
    "    words = [w for w in txt.split(' ') if w!='']\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print get_keywords(txt2words1(my_document['body']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ª versión. Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Taken from NLTK corpus so you don't have to download\n",
    "\n",
    "#import nltk\n",
    "#nltk.download()\n",
    "#from nltk.corpus import stopwords\n",
    "#print filter_symbols(\n",
    "#    ' '.join(\n",
    "#        sorted(stopwords.words('spanish'))\n",
    "#    )\n",
    "#)\n",
    "\n",
    "STOPWORDS = set('''\n",
    "a al algo algunas algunos ante antes como con contra cual cuando de del desde donde durante e el ella ellas ellos\n",
    "en entre era erais eran eras eres es esa esas ese eso esos esta estaba estabais estaban estabas estad estada estadas\n",
    "estado estados estamos estando estar estaremos estara estaran estaras estare estareis estaria estariais estariamos\n",
    "estarian estarias estas este estemos esto estos estoy estuve estuviera estuvierais estuvieran estuvieras estuvieron\n",
    "estuviese estuvieseis estuviesen estuvieses estuvimos estuviste estuvisteis estuvieramos estuviesemos estuvo esta\n",
    "estabamos estais estan estas este esteis esten estes fue fuera fuerais fueran fueras fueron fuese fueseis fuesen fueses\n",
    "fui fuimos fuiste fuisteis fueramos fuesemos ha habida habidas habido habidos habiendo habremos habra habran habras\n",
    "habre habreis habria habriais habriamos habrian habrias habeis habia habiais habiamos habian habias han has hasta\n",
    "hay haya hayamos hayan hayas hayais he hemos hube hubiera hubierais hubieran hubieras hubieron hubiese hubieseis\n",
    "hubiesen hubieses hubimos hubiste hubisteis hubieramos hubiesemos hubo la las le les lo los me mi mis mucho muchos\n",
    "muy mas mi mia mias mio mios nada ni no nos nosotras nosotros nuestra nuestras nuestro nuestros o os otra otras otro\n",
    "otros para pero poco por porque que quien quienes que se sea seamos sean seas sentid sentida sentidas sentido sentidos\n",
    "seremos sera seran seras sere sereis seria seriais seriamos serian serias seais siente sin sintiendo sobre sois somos\n",
    "son soy su sus suya suyas suyo suyos si tambien tanto te tendremos tendra tendran tendras tendre tendreis tendria\n",
    "tendriais tendriamos tendrian tendrias tened tenemos tenga tengamos tengan tengas tengo tengais tenida tenidas tenido\n",
    "tenidos teniendo teneis tenia teniais teniamos tenian tenias ti tiene tienen tienes todo todos tu tus tuve tuviera\n",
    "tuvierais tuvieran tuvieras tuvieron tuviese tuvieseis tuviesen tuvieses tuvimos tuviste tuvisteis tuvieramos tuviesemos\n",
    "tuvo tuya tuyas tuyo tuyos tu un una uno unos vosostras vosostros vuestra vuestras vuestro vuestros y ya yo el eramos\n",
    "'''.split())\n",
    "\n",
    "def txt2words2(txt):\n",
    "    words = [w for w in txt.split(' ') if w!='' and w not in STOPWORDS]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print get_keywords(txt2words2(my_document['body']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ª versión. Quitando mayúsculas, tildes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def txt2words3(txt):\n",
    "    txt = txt.lower()  # Text in lowercase\n",
    "    table = dict(zip( #  Quitar tildes\n",
    "        [ord(x) for x in u'áéíóúü'],\n",
    "        [ord(x) for x in u'aeiouu']\n",
    "    ))\n",
    "    txt = txt.translate(table)    \n",
    "    txt = ''.join([\n",
    "        letter for letter in txt \n",
    "        if letter in set(u'abcdefghijklmnñopqrstuvwxyz0123456789 ')]\n",
    "    )\n",
    "    words = [w for w in txt.split(' ') if w!='' and w not in STOPWORDS]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print get_keywords(txt2words3(my_document['body']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ª versión. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SpanishStemmer\n",
    "stemmer = SpanishStemmer()\n",
    "\n",
    "def txt2words4(txt):\n",
    "    txt = txt.lower()  # Text in lowercase\n",
    "    table = dict(zip( #  Quitar tildes\n",
    "        [ord(x) for x in u'áéíóúü'],\n",
    "        [ord(x) for x in u'aeiouu']\n",
    "    ))\n",
    "    txt = txt.translate(table)    \n",
    "    txt = ''.join([\n",
    "        letter for letter in txt \n",
    "        if letter in set(u'abcdefghijklmnñopqrstuvwxyz0123456789 ')]\n",
    "    )\n",
    "    words = [\n",
    "        stemmer.stem(w)\n",
    "        for w in txt.split(' ')\n",
    "        if w!='' and w not in STOPWORDS\n",
    "    ]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print get_keywords(txt2words4(my_document['body']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicándolo a todos los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for d in documents:\n",
    "    for key in ['body', 'summary']:\n",
    "        d[key+'_tokens'] = txt2words4(d[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Film titles\n",
    "titles = sorted(list({d['title'] for d in documents}))\n",
    "print '\\n'.join(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def explore(title):\n",
    "    print \"Título: {}\".format(title)\n",
    "    for kws in [get_keywords(d['body_tokens']) for d in documents if title==d['title']]:\n",
    "        print ' ->  {}'.format(', '.join(kws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "explore(u'Harry Potter y la piedra filosofal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "explore('High School Musical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explore('Los puentes de Madison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explore('2001, Odisea del espacio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "explore('Alien vs Predator 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "explore('Apocalypto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportar/Importar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import zlib\n",
    "\n",
    "\n",
    "# Export\n",
    "with open(config.DATASET_MUCHOCINE, 'w+') as fd:\n",
    "    fd.write(zlib.compress(json.dumps(documents)))\n",
    "    \n",
    "# Import\n",
    "with open(config.DATASET_MUCHOCINE, 'r') as fd:\n",
    "    docs_import =json.loads(zlib.decompress(fd.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
