{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Palabras clave (1ª versión)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frecuencia de aparición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_keywords(words):\n",
    "    word_counts = defaultdict(lambda : 0)\n",
    "    for w in words:\n",
    "        word_counts[w]+= 1\n",
    "    return [ x for x,_ in sorted(word_counts.iteritems(), key=lambda (_,freq):freq, reverse=True)][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset URL: http://www.lsi.us.es/~fermin/corpusCine.zip\n",
    "\n",
    "Nos quedamos con los ficheros .xml, que tienen una estructura del estilo:\n",
    "\n",
    "```xml\n",
    "<review author=\"XXX\" title=\"XXX\" rank=\"X\" maxRank=\"X\" source=\"XXX\">\n",
    "\t<summary>XXX</summary>\n",
    "\t<body>XXX</body>\n",
    "</review>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detección de la codificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "with open(config.DATASET_MUCHOCINE_RAW+'/100.xml', 'r') as fd:\n",
    "    txt = fd.read()\n",
    "    \n",
    "print chardet.detect(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parseado con expresiones regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs  # Know your encoding\n",
    "import re\n",
    "\n",
    "def parse_file(file_path):\n",
    "    retval = {'file': os.path.basename(file_path)}\n",
    "    with codecs.open(file_path,'r', encoding='ISO-8859-2') as fd:\n",
    "        f = fd.read().strip()\n",
    "    reg_expr = re.compile(r'\\<review author=\"(?P<author>.*)\" title=\"(?P<title>.*)\" rank=\"(?P<rank>\\d)\".*\\>\\s*<summary>(?P<summary>.*)</summary>\\s*<body>(?P<body>.*)</body>\\s*</review>')\n",
    "    regexp_result = reg_expr.search(f.strip())\n",
    "    for key in ['author', 'rank', 'title', 'summary', 'body']:\n",
    "        retval[key] = regexp_result.group(key)\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "for file_name in os.listdir(config.DATASET_MUCHOCINE_RAW):\n",
    "    try:\n",
    "        documents.append(\n",
    "            parse_file(config.DATASET_MUCHOCINE_RAW+'/'+file_name)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print file_name\n",
    "        print e.message\n",
    "documents.sort(key=lambda x: x['file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ª versión. Contar palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def txt2words(txt):\n",
    "    words = [w for w in txt.split(' ') if w!='']\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print get_keywords(txt2words(documents[0]['body']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ª versión. Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Taken from NLTK corpus so you don't have to download\n",
    "\n",
    "#import nltk\n",
    "#nltk.download()\n",
    "#from nltk.corpus import stopwords\n",
    "#print filter_symbols(\n",
    "#    ' '.join(\n",
    "#        sorted(stopwords.words('spanish'))\n",
    "#    )\n",
    "#)\n",
    "\n",
    "STOPWORDS = set('''\n",
    "a al algo algunas algunos ante antes como con contra cual cuando de del desde donde durante e el ella ellas ellos\n",
    "en entre era erais eran eras eres es esa esas ese eso esos esta estaba estabais estaban estabas estad estada estadas\n",
    "estado estados estamos estando estar estaremos estara estaran estaras estare estareis estaria estariais estariamos\n",
    "estarian estarias estas este estemos esto estos estoy estuve estuviera estuvierais estuvieran estuvieras estuvieron\n",
    "estuviese estuvieseis estuviesen estuvieses estuvimos estuviste estuvisteis estuvieramos estuviesemos estuvo esta\n",
    "estabamos estais estan estas este esteis esten estes fue fuera fuerais fueran fueras fueron fuese fueseis fuesen fueses\n",
    "fui fuimos fuiste fuisteis fueramos fuesemos ha habida habidas habido habidos habiendo habremos habra habran habras\n",
    "habre habreis habria habriais habriamos habrian habrias habeis habia habiais habiamos habian habias han has hasta\n",
    "hay haya hayamos hayan hayas hayais he hemos hube hubiera hubierais hubieran hubieras hubieron hubiese hubieseis\n",
    "hubiesen hubieses hubimos hubiste hubisteis hubieramos hubiesemos hubo la las le les lo los me mi mis mucho muchos\n",
    "muy mas mi mia mias mio mios nada ni no nos nosotras nosotros nuestra nuestras nuestro nuestros o os otra otras otro\n",
    "otros para pero poco por porque que quien quienes que se sea seamos sean seas sentid sentida sentidas sentido sentidos\n",
    "seremos sera seran seras sere sereis seria seriais seriamos serian serias seais siente sin sintiendo sobre sois somos\n",
    "son soy su sus suya suyas suyo suyos si tambien tanto te tendremos tendra tendran tendras tendre tendreis tendria\n",
    "tendriais tendriamos tendrian tendrias tened tenemos tenga tengamos tengan tengas tengo tengais tenida tenidas tenido\n",
    "tenidos teniendo teneis tenia teniais teniamos tenian tenias ti tiene tienen tienes todo todos tu tus tuve tuviera\n",
    "tuvierais tuvieran tuvieras tuvieron tuviese tuvieseis tuviesen tuvieses tuvimos tuviste tuvisteis tuvieramos tuviesemos\n",
    "tuvo tuya tuyas tuyo tuyos tu un una uno unos vosostras vosostros vuestra vuestras vuestro vuestros y ya yo el eramos\n",
    "'''.split())\n",
    "\n",
    "def txt2words(txt):\n",
    "    words = [w for w in txt.split(' ') if w!='' and w not in STOPWORDS]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print get_keywords(txt2words(documents[0]['body']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ª versión. Quitando mayúsculas, tildes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def txt2words(txt):\n",
    "    txt = txt.lower()  # Text in lowercase\n",
    "    table = dict(zip( #  Quitar tildes\n",
    "        [ord(x) for x in u'áéíóúü'],\n",
    "        [ord(x) for x in u'aeiouu']\n",
    "    ))\n",
    "    txt = txt.translate(table)    \n",
    "    txt = ''.join([\n",
    "        letter for letter in txt \n",
    "        if letter in set(u'abcdefghijklmnñopqrstuvwxyz0123456789 ')]\n",
    "    )\n",
    "    words = [w for w in txt.split(' ') if w!='' and w not in STOPWORDS]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print get_keywords(txt2words(documents[0]['body']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ª versión. Raíces léxicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SpanishStemmer\n",
    "stemmer = SpanishStemmer()\n",
    "\n",
    "def txt2words(txt):\n",
    "    txt = txt.lower()  # Text in lowercase\n",
    "    table = dict(zip( #  Quitar tildes\n",
    "        [ord(x) for x in u'áéíóúü'],\n",
    "        [ord(x) for x in u'aeiouu']\n",
    "    ))\n",
    "    txt = txt.translate(table)    \n",
    "    txt = ''.join([\n",
    "        letter for letter in txt \n",
    "        if letter in set(u'abcdefghijklmnñopqrstuvwxyz0123456789 ')]\n",
    "    )\n",
    "    words = [\n",
    "        stemmer.stem(w)\n",
    "        for w in txt.split(' ')\n",
    "        if w!='' and w not in STOPWORDS\n",
    "    ]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print get_keywords(txt2words(documents[0]['body']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicándolo a todos los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for d in documents:\n",
    "    for key in ['body', 'summary']:\n",
    "        d[key+'_tokens'] = txt2words(d[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Film titles\n",
    "titles = sorted(list({d['title'] for d in documents}))\n",
    "print '\\n'.join(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def explore(title):\n",
    "    print title\n",
    "    for kws in [get_keywords(d['body_tokens']) for d in documents if title==d['title']]:\n",
    "        print ', '.join(kws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "explore('Rambo: acorralado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "explore('Saw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explore('Los puentes de Madison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explore('2001, Odisea del espacio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "explore('Apocalypto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportar/Importar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import zlib\n",
    "\n",
    "'''\n",
    "# Keys to keep\n",
    "keys_export = ['file', 'title', 'author','rank', 'summary_tokens', 'body_tokens']\n",
    "# New data structure to export\n",
    "docs_export = [\n",
    "    {key: d[key] for key in keys_export}\n",
    "    for d in documents\n",
    "]'''\n",
    "\n",
    "# Export\n",
    "with open(config.DATASET_MUCHOCINE, 'w+') as fd:\n",
    "    fd.write(zlib.compress(json.dumps(documents)))\n",
    "    \n",
    "# Import\n",
    "with open(config.DATASET_MUCHOCINE, 'r') as fd:\n",
    "    docs_import =json.loads(zlib.decompress(fd.read()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
